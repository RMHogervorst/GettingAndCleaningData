---
title: "getting and cleaning data"
author: "Roel Hogervorst MSc"
date: "Sunday, March 08, 2015"
output: html_document
---
bijgevoegd:alle slides per week.
#week 1.Openen van data


Feitelijk wil Jeffrey T leek dat je een getting and cleaning data script maakt dat start en doorloopt van raw data naar clean data. En elke keer opnieuw moet dat hetzeflde lopen.

handig scriptje voor herhaaldelijk doorlopen van hele script. Vaak wil je niet elke keer opnieuw directories maken.
Daarom dit scriptje.
```{r create directory if not yet exists}
if (!file.exists("data")) {
        dir.create("data")
        }
```
Voor het downloaden van bestanden gebruik download.file() met belangrijke argumenten url, destfile, method
download.file interesseert het niet wat het download. het download wat je wilt.
```{r download file spec voor opdracht week 1}
#eerst de link opslaan,dan downloaden via link, vervolgens downloaddatum opslaan voor later.
fileUrl<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile="./data/microdatasurvey.csv") #in windows geen method curl gebruiken.
#ook codebook downloaden.
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf", destfile="./data/codebook.pdf") #dit werkt niet.
dataDownloaded<-date()

```
dan openen als dataframe. read.table() of gerelateerd: read.csv() of read.csv2() belangrijke paramaters: file, header, sep, row.names, nrows.
```{r openen van local file}
microdata<-read.csv("./data/microdatasurvey.csv")
```
How many properties are worth $1,000,000 or more? 
Hiervoor gebruik ik de VAL variabele. dat is nr 14 of meer. eerst checken welke waarden die variabele aanneemt.
dat is fout, want ik heb nu 100.000 of meer. juiste nummer is 24. 
```{r opdrachten week 1 over microdata}
unique(microdata$VAL)
sum(microdata$VAL >14, na.rm=T) #deze is fout
sum(microdata$VAL >=24, na.rm=T) # deze is goed.
```
volgende opdracht: download excel sheet.
```{r downloaden van excelfile}
xlUrl<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(xlUrl, destfile="./data/NGAP.xlsx", mode="wb")
```
extracting data from excel files.


```{r opening excel files}
library(xlsx)
#rijen 18-23 en kollommen 7-15
colIndex<-18:23
rowIndex<-1:4
dat<-read.xlsx("data/NGAP.xlsx",sheetIndex=1, colIndex=colIndex, rowIndex=rowIndex, header=T)# dit werkt niet.
```

```{r manuele variant openen}
library(xlsx)
#rijen 18-23 en kollommen 7-15
dat<-read.xlsx("./data/NGAP.xlsx",sheetIndex=1, header=T)
dat<-dat[18:22,]
dat<-dat[,7:15]
sum(dat$Zip*dat$Ext,na.rm=T)
```

```{r laden van xml files}
library(XML) #bibliotheek laden
XML_url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml" #link opslaan
download.file(XML_url, destfile="./data/restaurants.xml")
doc<-xmlTreeParse("./data/restaurants.xml",useInternal=T)#openen van bestand
```

```{r via downloader}
library(downloader)
library(XML)
library(Rcurl)
XML_url<-"http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
download(XML_url, destfile= "data/restaurants.xml")
doc<-xmlParse("./data/restaurants.xml")
```        
ik krijg het niet voor elkaar en de helpfuncties zijn ook waardeloos. Alternatief was om het xml bestand in notepad ++ te openen en te tellen. hoeveel postcodes er zijn.         

download microsurvey data over state of idaho.
``` {r download idaoho file}
link<-"http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv "
download.file(link, destfile= "data/idaho.csv")
require(data.table)
DT<-fread("data/idaho.csv")
```
Which of the following is the fastest way to calculate the average value of the variable

pwgtp15 

broken down by sex using the data.table package? 

------

gebruik system.time kan niet want data.table is geschreven in C. dus het neemt geen tijd in beslag in de system.time.
Ik heb gebruik  gemaakt van de data.table video file.

##dplyr lessen uit swirl.
 One unique aspect of dplyr is that the same set of tools allow you to work with tabular data from a variety of sources, including data frames, data tables, databases and multidimensional arrays. In this lesson, we'll focus on data frames, but everything you learn will apply equally to other formats.

I've created a variable called path2csv, which contains the full file path to the
dataset. Call read.csv() with two arguments, path2csv and stringsAsFactors = FALSE,
and save the result in a new variable called mydf. Check ?read.csv if you need help.

mydf<-read.csv(path2csv, stringsAsFactors =FALSE)
 
 MAAKT GEBRUIK VAN SPECIALE VORM tbl_df
 dus bijvoorbeeld: cran<-tbl_df(mydf)
 dplyr print ook nettere weergaven van je data. gewoon de erste 10 en alleen wat in je console past.
 
According to the "Introduction to dplyr" vignette written by the package authors, "The dplyr philosophy is to have small functions that each do one thing well."
Specifically, dplyr supplies five 'verbs' that cover most fundamental data
manipulation tasks: select(), filter(), arrange(), mutate(), and summarize().

select(dataframe, waarop je wilt selecteren, enz) doet de kolommen.
filter(dataframe, waarop je wilt filteren)  doet de rijen.
arrange()veranderd de volgorde
mutate() voegt kolommen toe, vervangt ze
summarize() geeft nette samenvatting van data.
 
reading excel, xml, json.

#week2
reading from MySQL, HDF5, the web, API's, other. 
###MySQL
MySQL is open source database, veel gebruikt in internetapplicaties. data is gestructureerd in Databases, en tabellen in die databases, met daarin velden. elke rij wordt een 'record' genoemd. 
Meer informatie over installeren van MySQL in windows (http://www.ahschulz.de/2013/07/23/installing-rmysql-under-windows/).
KEnker het installeren van deze tools kost echt veel tijd. 
Altijd connectie maken en weer stoppen wanneer je klaar bent.
###HDF5
HDF5 is een set bestandsformaten om grote hoeveelheden numerieke data op te slaan en te organizeren. Wrodt gebruikt voor supercomputers en andere dataintensieve zaken. HDF5 is ondersteund door Java, MATLAB, OCTAVE, IDL, Python,R, en Julia. Waarom het zo awesome is, is me niet duidelijk, maar het lijkt sneller te zijn dan MySQL.
###reading from the web
Je kunt bijvoorbeeld data van websites afhalen. 
bijvoorbeeld:
```{r}
#con=url("http://enz.")
#htmlCode= readLines(con)
#close(con)
#htmlCode
```
Wordt een string met inhoud van de webpagina.
handiger is dan om die string met de XML package te openen.

###api's


###Other files
meestal is er een package voor wat je wilt.
algemeen
file - open een connectie naar tekst file.
url - open een verbinding naar een url
gzfile open een connectie naar .gz bestand.
bzfile open connectie naar .bz2file.
?connections voor meer informatie.
SLUIT VERBINDINGEN NA AFLOOP!

package: foreign leest van minitab, s, sas, spss, stata, systat.
heeft dan ook reas.spss oa.
deze presentatie heeft veel informatie over verschillende file packages.
##opdrachten
wanneer is datasharing gemaakt? 
Ik heb niet gekloot met apis zoals zou mouten, maar ik heb gewoon naar de eerste commit gekeken.

``` {r download americak commonity file}
link<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(link, destfile= "data/getdata_ss06p.csv")
```
dan inladen.
```{r inladen dataset}
acs<-read.csv("data/getdata_ss06p.csv")
```
```{r analyse}
library(sqldf)
#sqldf("select pwgtp1 from acs where AGEP < 50")
#sqldf("select pwgtp1 from acs where AGEP < 50")
unique(acs$AGEP)
sqldf("select distinct AGEP from acs") #is gelijk aan unique query.
```

volgende stap is het openen van webpagina.
```{r lezen webpagina}
con=url("http://biostat.jhsph.edu/~jleek/contact.html ")
htmlCode=readLines(con)
close(con) # belangrijk om connection te sluiten na afloop.
```
dan de data manipuleren
```{r manipuleren van data}
nchar(htmlCode[10])
nchar(htmlCode[20])
nchar(htmlCode[30])
nchar(htmlCode[100])
```

```{r read }
read.fwf("data/getdata_wksst8110.for")

```
